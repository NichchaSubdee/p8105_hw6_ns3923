---
title: "p8105_hw6_ns3923"
output: github_document
---

**Set up necessary libraries**
```{r setup, message=FALSE, warning=FALSE}
library(tidyverse)
library(broom)
library(p8105.datasets)
library(modelr)
```

## Problem 1: The Washington Post - homicides 

**1) Import dataset**
```{r P1_import, message = FALSE}
homicide_df = read_csv("./data/homicide-data.csv") |> 
  janitor::clean_names()

homicide_df
```

**2) Cleaning and tidying data**

* Create a city_state variable (e.g. “Baltimore, MD”), and a binary variable indicating whether the homicide is solved. 
* Omit cities Dallas, TX; Phoenix, AZ; and Kansas City, MO (don’t report victim race), and omit Tulsa, AL (a data entry mistake). 
* Limit those for whom victim_race is white or black, and victim_age is numeric.

```{r P1_clean, message = FALSE, warning=FALSE}
homicide_df = homicide_df |> 
  mutate(
    city_state = str_c(city, state, sep = ", "),
    resolved   = if_else(disposition == "Closed by arrest", 1, 0),
    victim_age = as.numeric(victim_age)
  ) |> 
  
  filter(
    !city_state %in% c("Dallas, TX", "Phoenix, AZ", 
                       "Kansas City, MO", "Tulsa, AL"),
    victim_race %in% c("White", "Black")
  ) |> 
  drop_na(victim_age, victim_sex, victim_race)

##Check
homicide_df 
```

**3) Logistic regression for Baltimore, MD**

* Use the glm function to fit a logistic regression with resolved vs unresolved as the outcome and victim age, sex and race as predictors. 

```{r P1_baltimore, message = FALSE}
baltimore_df = 
  homicide_df |> 
  filter(city_state == "Baltimore, MD")

baltimore_fit = 
  baltimore_df |> 
  glm(
    resolved ~ victim_age + victim_sex + victim_race,
    data   = _,
    family = binomial()
  )

baltimore_or = 
  baltimore_fit |> 
  tidy(conf.int = TRUE, exponentiate = TRUE) |> 
  filter(term == "victim_sexMale") |> 
  mutate(term = "Male vs Female (ref)") |> 
  select(term, estimate, conf.low, conf.high)

baltimore_or |> 
  knitr::kable(digits = 3)
```

**Table 1: the estimated OR and 95% CI of resolved vs unresolved of male compared to female victims**

**Explanation:** In Baltimore, MD, after adjusting for victim age and race, homicides with male victims have lower odds of being resolved compared to homicides with female victims (adjusted OR = 0.426; 95% CI: 0.324, 0.558). Because the confidence interval doesn't include 1, there is significant evidence that cases involving male victims are less likely to be solved.

**4) Adjusted OR and CI for solving homicides comparing male to female victims**

```{r P1_OR, message=FALSE, warning=FALSE}
city_results = 
  homicide_df |> 
  nest(data = -city_state) |> 
  mutate(
    models = map(
      data,
      ~ glm(resolved ~ victim_age + victim_sex + victim_race,
            data = ., family = binomial())
    ),
    tidy_res = map(
      models,
      ~ tidy(., conf.int = TRUE, exponentiate = TRUE)
    )
  ) |> 
  select(city_state, tidy_res) |> 
  unnest(tidy_res) |> 
  filter(term == "victim_sexMale") |> 
  rename(
    or       = estimate,
    or_lower = conf.low,
    or_upper = conf.high
  )

city_results |> 
  arrange(or) |> 
  select(city_state, or, or_lower, or_upper) |> 
  mutate(
    or       = round(or, 3),
    or_lower = round(or_lower, 3),
    or_upper = round(or_upper, 3)
  ) |> 
  knitr::kable(
    col.names = c(
      "City",
      "Adjusted OR (Male vs Female)",
      "Lower 95% CI",
      "Upper 95% CI"
    )
  )
```

**Table 2: The adjusted OR (Male vs Female) and 95% CI for solving homicides**

**5) Create a plot shows the estimated ORs and CIs for each city**

```{r P1_plot, message=FALSE, fig.height=8}
city_results |> 
  mutate(city_state = fct_reorder(city_state, or)) |> 
  ggplot(aes(x = city_state, y = or)) +
  geom_point(color = "red") +
  geom_errorbar(aes(ymin = or_lower, ymax = or_upper), width = 0.5) +
  geom_hline(yintercept = 1, linetype = "dashed") +
  coord_flip() +
  labs(
    title = "Figure 1: Adjusted ORs for solving homicides (Male vs Female)",
    x = "City",
    y = "Adjusted OR"
  ) +
  theme_minimal()
```

**Interpretation:** Most cities have odds ratios below 1, meaning homicides involving male victims are less likely to be solved than those involving female victims, after adjusting for victim age and race. Only a few cities, such as Albuquerque, NM, Stockton, CA, and Fresno, CA, have odds ratios above 1. Still, their confidence intervals are wide, so the results are uncertain, and we cannot be sure whether solving rates are truly higher among male than female victims. These uncertain exceptions may reflect a smaller population or more variable case patterns in those cities. The lowest odds ratios are in New York, NY, Baton Rouge, LA, and Omaha, NE. These cities show a much lower chance of solving cases involving male victims. 

## Problem 2: Central Park weather data

**1) Import data**
```{r P2_import, message=FALSE}
data("weather_df")

set.seed(123)
```

We’ll focus on a simple linear regression with `tmax` as the response with `tmin` and `prcp` as the predictors, and are interested in the distribution of two quantities estimated from these data:
* Estimated R-squared
* Ratio of estimated beta_1/beta_2 (β1/β2)

**2) Clean dataset**

```{r P2_clean, message=FALSE}
weather_df_clean = weather_df |>
  drop_na(tmax, tmin, prcp)

n_obs = nrow(weather_df_clean)
```

**3) Create 5,000 bootstrap samples and fit the model in each sample**

```{r P2_bootstrap, message=FALSE}
weather_bootstrap = weather_df_clean |> 
  bootstrap(5000, id = "strap_id") |> 
  mutate(
    model = map(strap, ~ lm(tmax ~ tmin + prcp, data = .x)),
    glance = map(model, glance),
    tidy = map(model, tidy)
  )
```

**4) The estimated r-squared (r^2) and beta_1 and beta_2 (β1/β2)**

```{r P2_extract, message=FALSE}
boot_results = weather_bootstrap |> 
  transmute(
    strap_id,
    r_sq = map_dbl(glance, "r.squared"),
    beta_1 = map_dbl(
      tidy,
      ~ .x |> 
        filter(term == "tmin") |> 
        pull(estimate)
    ),
    beta_2 = map_dbl(
      tidy,
      ~ .x |> 
        filter(term == "prcp") |> 
        pull(estimate)
    ),
    beta_ratio = beta_1 / beta_2
  )

boot_results

```

**5) Plot the distribution of the estimates r-squared and beta1/beta2 (β1/β2)**

```{r P2_plot, message=FALSE}
boot_results |> 
  ggplot(aes(x = r_sq)) +
  geom_histogram(bins = 30, fill = "skyblue", color = "white") +
  labs(
    title = "Figure 2: Bootstrap distribution of estimated R-squared",
    x = expression(hat(r)^2),
    y = "Count"
  ) +
  theme_minimal()

boot_results |> 
  ggplot(aes(x = beta_ratio)) +
  geom_histogram(bins = 30, fill = "orange", color = "white") +
  labs(
    title = "Figure 3: Bootstrap distribution of β1/β2",
    x = expression(hat(beta)[1] / hat(beta)[2]),
    y = "Count"
  ) +
  theme_minimal()

```

**Explanation:** The bootstrap distribution of the estimated R-squared is centered around 0.94, indicating the model consistently explains most of the variation in `tmax` across samples. In addition, the distribution of the ratio β1/β2 is entirely negative and more spread out. This happens because `tmin` has a strong positive effect on `tmax`, while
`prcp` has a small negative effect. Overall, this shows that the effect of `tmin` is very stable, but the small size of the `prcp` coefficient leads to more variability in the ratio.

**6) Identify the 2.5% and 97.5% quantiles to provide a 95% confidence interval for r^2 and β1/β2**

```{r P2_CI, message=FALSE}
boot_ci =
  boot_results |>
  summarize(
    r_sq_lower  = quantile(r_sq, 0.025),
    r_sq_upper  = quantile(r_sq, 0.975),
    ratio_lower = quantile(beta_ratio, 0.025),
    ratio_upper = quantile(beta_ratio, 0.975)
  )

boot_ci |> 
  knitr::kable(digits = 3,
               col.names = c(
                 "R² lower",
                 "R² upper",
                 "β₁/β₂ lower",
                 "β₁/β₂ upper"
               ))
```

**Explanation:** The 95% bootstrap confidence interval for R-squared (0.934 to 0.947) shows that the model fit is very stable and consistently explains more than 93% of the variation in `tmax`. For the ratio β1/β2, the confidence interval (–277.17 to –125.71) is negative, which matches the signs of the estimated slopes. This means that `tmin` has a strong positive effect on `tmax`, while `prcp` has a small negative effect across samples.


## Problem 3: Birthweight

**1) Import dataset**
```{r P3_import, message = FALSE}
bw_df = read_csv("./data/birthweight.csv") |> 
  janitor::clean_names()

bw_df
```

Load and clean the data for regression analysis (i.e. use appropriate variable names, convert numeric to factor where appropriate, check for the presence of missing data, etc.).

Propose a regression model for birthweight. This model may be based on a hypothesized structure for the factors that underly birthweight, on a data-driven model-building process, or a combination of the two. Describe your modeling process and show a plot of model residuals against fitted values – use add_predictions and add_residuals in making this plot.

Compare your model to two others:

One using length at birth and gestational age as predictors (main effects only)
One using head circumference, length, sex, and all interactions (including the three-way interaction) between these
Make this comparison in terms of the cross-validated prediction error; use crossv_mc and functions in purrr as appropriate.

Note that although we expect your model to be reasonable, model building itself is not a main idea of the course and we don’t necessarily expect your model to be “optimal”.